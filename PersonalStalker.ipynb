{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPAi1kfJLpn94C3kKrZ05Vj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LinaTobon/Proyecticos-/blob/main/PersonalStalker.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Personal Stalker\n",
        "En este proyecto aprenderemos a investigar y obtener los temas principales de los cuales publica una cuenta de Twitter que te interece.\n",
        "De esta manera podrás tener un acercamiento a los aspectos que motivan a esta persona o empresa."
      ],
      "metadata": {
        "id": "24eWUsZM_-6h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fases\n",
        "1. Configuraciones, definiciones e importación de librerías\n",
        "2. Declaración de funciones para:\n",
        "   - Conexión a la API de twitter\n",
        "   - Extracción de tweets de la cuenta elegida\n",
        "   - Limpieza y transformación de datos.\n",
        "5. Implementar modelo de tópicos\n",
        "6. Visualización\n",
        "7. Insights"
      ],
      "metadata": {
        "id": "m_qERxgoAgTQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. Configuraciones, definiciones e importación de librerías**"
      ],
      "metadata": {
        "id": "9ZAcVRI9BGgV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\") # Para ignoarar notificaciones molestas\n",
        "\n",
        "# Instalación de paquetes\n",
        "!pip install contractions\n",
        "!pip install textsearch\n",
        "!pip install tqdm\n",
        "!pip install nltk\n",
        "!pip install spacy\n",
        "\n",
        "# importando librarias\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import collections\n",
        "import itertools\n",
        "import plotly.express as px\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import contractions\n",
        "\n",
        "import tweepy \n",
        "\n",
        "import spacy\n",
        "from spacy.lang.en import English\n",
        "#spacy.load('en')\n",
        "\n",
        "\n",
        "import nltk\n",
        "from nltk.util import ngrams\n",
        "from nltk.collocations import BigramCollocationFinder\n",
        "from nltk.metrics import BigramAssocMeasures\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "en_stop = set(nltk.corpus.stopwords.words('english'))\n",
        "\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "\n",
        "from gensim import corpora\n",
        "import pickle\n",
        "import gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B1TFR7AQBjfB",
        "outputId": "e6775059-d1d4-4441-d257-f1dc4bbeeb19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: contractions in /usr/local/lib/python3.8/dist-packages (0.1.73)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.8/dist-packages (from contractions) (0.0.24)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.8/dist-packages (from textsearch>=0.0.21->contractions) (1.4.4)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.8/dist-packages (from textsearch>=0.0.21->contractions) (0.3.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: textsearch in /usr/local/lib/python3.8/dist-packages (0.0.24)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.8/dist-packages (from textsearch) (0.3.1)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.8/dist-packages (from textsearch) (1.4.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (4.64.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (3.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from nltk) (4.64.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk) (2022.6.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.8/dist-packages (3.4.4)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.0.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy) (6.3.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (8.1.5)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.4.5)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.10.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (4.64.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.7.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (21.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.21.6)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->spacy) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.4.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy) (2.0.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- Settings ----#\n",
        "\n",
        "#Twitter API credentials\n",
        "\n",
        "consumer_key = \"uP50Js2WwFNEllApoOS015PLb\"\n",
        "consumer_secret = \"se7bTiiu24IwIXUg8qRhy4fXh6hPcsmgRMrvjj6kUfKP3tOHEG\"\n",
        "access_key = \"963516449418563585-SYiUbAl4qfYiiaOUTmSJ0WX32GGj94J\"\n",
        "access_secret=\"IDoGQ3KeNjScSzk42Rt7HQTJMKazfm3CgEh8qFlJnTnNb\"\n",
        "\n",
        "# Definimos stop_words\n",
        "stop_words = nltk.corpus.stopwords.words('english')\n",
        "stop_words_spanish = nltk.corpus.stopwords.words('spanish')\n",
        "\n",
        "# inicializamos stemmer y lemmatizer\n",
        "ps = nltk.porter.PorterStemmer()\n",
        "wnl = WordNetLemmatizer()\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "7_bwci3TDJba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. Declaración de funciones**"
      ],
      "metadata": {
        "id": "BS86oorXCrda"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#---- Conectandonos a la API de Twitter  y extrayendo datos ----#\n",
        "\n",
        "def get_all_tweets(screen_name):\n",
        "\n",
        "    \"\"\"funcion que permite descargar los 3240 tweets mas recientes de la cuenta especificada\"\"\"\n",
        "    \n",
        "    # Inicializar tweepy\n",
        "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "    auth.set_access_token(access_key, access_secret)\n",
        "    api = tweepy.API(auth)\n",
        "    \n",
        "    # Declarar una lista para almacenar losTweets\n",
        "    alltweets = []  \n",
        "    \n",
        "    # Request para obtener los 200 tweets mas recientes\n",
        "    new_tweets = api.user_timeline(screen_name = screen_name,count=200)\n",
        "    \n",
        "    # Concatenando el resultado del request\n",
        "    alltweets.extend(new_tweets)\n",
        "    \n",
        "    # obtenemos el id del tweet mas viejo que extrajimos \n",
        "    oldest = alltweets[-1].id - 1\n",
        "    \n",
        "    # Iteramos hasta que no haya mas tweets\n",
        "    while len(new_tweets) > 0:\n",
        "        \n",
        "        # Request para obtener los tweets anteriores al oldest\n",
        "        new_tweets = api.user_timeline(screen_name = screen_name,count=200,max_id=oldest)\n",
        "        \n",
        "        alltweets.extend(new_tweets)\n",
        "        \n",
        "        # actualizamos el id del elemento mas antiguo\n",
        "        oldest = alltweets[-1].id - 1\n",
        "        \n",
        "        print(f\"...{len(alltweets)} tweets descargados\")\n",
        "    \n",
        "    # Creamos un dataframe para almacenar los datos extraidos\n",
        "    df_tweets = pd.DataFrame(columns=[\"id\",\"created_at\",\"text\"])\n",
        "    df_tweets[\"id\"] =[tweet.id_str for tweet in alltweets]\n",
        "    df_tweets[\"user\"] =[tweet.user.id_str for tweet in alltweets]\n",
        "    df_tweets[\"created_at\"] =[tweet.created_at for tweet in alltweets]\n",
        "    df_tweets[\"text\"] =[tweet.text for tweet in alltweets]\n",
        "\n",
        "    return df_tweets\n",
        "\n",
        "\n",
        "#---- Limpieza de tweets ----#\n",
        "\n",
        "\n",
        "def tag_wordnet(tagged_tokens):\n",
        "    tag_map = {'j': wn.ADJ, 'v': wn.VERB, 'n': wn.NOUN, 'r': wn.ADV}\n",
        "    new_tagged_tokens = [(word, tag_map.get(tag[0].lower(), wn.NOUN))\n",
        "                            for word, tag in tagged_tokens]\n",
        "    return new_tagged_tokens\n",
        "\n",
        "def simple_text_preprocessor(document): \n",
        "    # lower case\n",
        "    document = str(document).lower()\n",
        "    \n",
        "    # expand contractions\n",
        "    document = contractions.fix(document)\n",
        "    \n",
        "    # remove unnecessary characters\n",
        "    document = re.sub(r'[^a-zA-Z]',r' ', document)\n",
        "    document = re.sub(r'nbsp', r'', document)\n",
        "    document = re.sub(' +', ' ', document)\n",
        "    document = re.sub(r'http\\S+', '',document)\n",
        "    \n",
        "    # simple porter stemming\n",
        "    #document = ' '.join([ps.stem(word) for word in document.split()])\n",
        "    tokens = nltk.word_tokenize(document)\n",
        "    tagged_tokens = nltk.pos_tag(tokens)\n",
        "    wordnet_tokens = tag_wordnet(tagged_tokens)\n",
        "    lemmatized_text = ' '.join(wnl.lemmatize(word, tag) for word, tag in wordnet_tokens)\n",
        "    lemmatized_tokens = nltk.word_tokenize(lemmatized_text)\n",
        "    \n",
        "    # stopwords removal\n",
        "    document = ' '.join([token for token in lemmatized_tokens if token not in stop_words and token not in stop_words_spanish ])\n",
        "    \n",
        "    return document\n",
        "\n",
        "def tokenize(text):\n",
        "    lda_tokens = []\n",
        "    parser = English()\n",
        "    tokens = parser(text)\n",
        "    for token in tokens:\n",
        "        if token.orth_.isspace():\n",
        "            continue\n",
        "        elif token.like_url:\n",
        "            lda_tokens.append('URL')\n",
        "        elif token.orth_.startswith('@'):\n",
        "            lda_tokens.append('SCREEN_NAME')\n",
        "        else:\n",
        "            lda_tokens.append(token.lower_)\n",
        "    return lda_tokens\n",
        "\n",
        "def get_lemma(word):\n",
        "    lemma = wn.morphy(word)\n",
        "    if lemma is None:\n",
        "        return word\n",
        "    else:\n",
        "        return lemma\n",
        "\n",
        "def prepare_text_for_lda(text):\n",
        "    tokens = tokenize(text)\n",
        "    tokens = [token for token in tokens if len(token) > 4]\n",
        "    tokens = [token for token in tokens if token not in en_stop]\n",
        "    tokens = [get_lemma(token) for token in tokens]\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "_wdreqkHCyMr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "historical_tweets = get_all_tweets('@Deloitte')\n",
        "stp = np.vectorize(simple_text_preprocessor)\n",
        "historical_tweets['clean_tweet'] = stp(historical_tweets['text'].values)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9F4Qe-tuI5Ec",
        "outputId": "b055fc09-9091-4a56-f6ac-6b91fac1aa3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "...400 tweets descargados\n",
            "...600 tweets descargados\n",
            "...800 tweets descargados\n",
            "...1000 tweets descargados\n",
            "...1200 tweets descargados\n",
            "...1400 tweets descargados\n",
            "...1600 tweets descargados\n",
            "...1800 tweets descargados\n",
            "...2000 tweets descargados\n",
            "...2200 tweets descargados\n",
            "...2400 tweets descargados\n",
            "...2600 tweets descargados\n",
            "...2800 tweets descargados\n",
            "...3000 tweets descargados\n",
            "...3200 tweets descargados\n",
            "...3250 tweets descargados\n",
            "...3250 tweets descargados\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "historical_tweets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 971
        },
        "id": "kv79fnZsKATV",
        "outputId": "12cc70a7-656c-4e41-cbb4-84944d397475"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                       id          created_at  \\\n",
              "0     1605639241039486976 2022-12-21 19:00:01   \n",
              "1     1605593945152622592 2022-12-21 16:00:01   \n",
              "2     1605578841489133568 2022-12-21 15:00:00   \n",
              "3     1605563747094986752 2022-12-21 14:00:01   \n",
              "4     1605548644006072321 2022-12-21 13:00:01   \n",
              "...                   ...                 ...   \n",
              "3245  1269316119971627008 2020-06-06 17:12:00   \n",
              "3246  1269282902342340613 2020-06-06 15:00:00   \n",
              "3247  1269041313028214786 2020-06-05 23:00:01   \n",
              "3248  1269002700064264195 2020-06-05 20:26:35   \n",
              "3249  1268996010568749062 2020-06-05 20:00:00   \n",
              "\n",
              "                                                   text     user  \\\n",
              "0     We proudly introduce you three of our Deloitte...  8457092   \n",
              "1     Want to join next year’s #OYW2023 cohort? Shar...  8457092   \n",
              "2     See how Deloitte scored  in @Gartner_inc. Crit...  8457092   \n",
              "3     Deloitte scored highest in 2 Use Cases in 2022...  8457092   \n",
              "4     Approx. 17% of people globally live with a dis...  8457092   \n",
              "...                                                 ...      ...   \n",
              "3245  From #technology and ethics to #leadership, #D...  8457092   \n",
              "3246  Did your organization adapt to the #blockchain...  8457092   \n",
              "3247  The new market landscape has challenged invest...  8457092   \n",
              "3248  Earlier this week we expressed our stance agai...  8457092   \n",
              "3249  Amidst the #COVID19 pandemic, how can the #ret...  8457092   \n",
              "\n",
              "                                            clean_tweet  \n",
              "0     proudly introduce three deloitte oyw ambassado...  \n",
              "1     want join next year oyw cohort share u help re...  \n",
              "2     see deloitte score gartner inc critical capabi...  \n",
              "3     deloitte score high use case gartner inc criti...  \n",
              "4     approx people globally live disability yet muc...  \n",
              "...                                                 ...  \n",
              "3245  technology ethic leadership deloittereview tac...  \n",
              "3246  organization adapt blockchain wave sign explor...  \n",
              "3247  new market landscape challenge investment bank...  \n",
              "3248  earlier week express stance racism inequality ...  \n",
              "3249  amidst covid pandemic retailbanking sector mov...  \n",
              "\n",
              "[3250 rows x 5 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-52929d80-587c-4629-b046-a5e3a73088aa\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>created_at</th>\n",
              "      <th>text</th>\n",
              "      <th>user</th>\n",
              "      <th>clean_tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1605639241039486976</td>\n",
              "      <td>2022-12-21 19:00:01</td>\n",
              "      <td>We proudly introduce you three of our Deloitte...</td>\n",
              "      <td>8457092</td>\n",
              "      <td>proudly introduce three deloitte oyw ambassado...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1605593945152622592</td>\n",
              "      <td>2022-12-21 16:00:01</td>\n",
              "      <td>Want to join next year’s #OYW2023 cohort? Shar...</td>\n",
              "      <td>8457092</td>\n",
              "      <td>want join next year oyw cohort share u help re...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1605578841489133568</td>\n",
              "      <td>2022-12-21 15:00:00</td>\n",
              "      <td>See how Deloitte scored  in @Gartner_inc. Crit...</td>\n",
              "      <td>8457092</td>\n",
              "      <td>see deloitte score gartner inc critical capabi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1605563747094986752</td>\n",
              "      <td>2022-12-21 14:00:01</td>\n",
              "      <td>Deloitte scored highest in 2 Use Cases in 2022...</td>\n",
              "      <td>8457092</td>\n",
              "      <td>deloitte score high use case gartner inc criti...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1605548644006072321</td>\n",
              "      <td>2022-12-21 13:00:01</td>\n",
              "      <td>Approx. 17% of people globally live with a dis...</td>\n",
              "      <td>8457092</td>\n",
              "      <td>approx people globally live disability yet muc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3245</th>\n",
              "      <td>1269316119971627008</td>\n",
              "      <td>2020-06-06 17:12:00</td>\n",
              "      <td>From #technology and ethics to #leadership, #D...</td>\n",
              "      <td>8457092</td>\n",
              "      <td>technology ethic leadership deloittereview tac...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3246</th>\n",
              "      <td>1269282902342340613</td>\n",
              "      <td>2020-06-06 15:00:00</td>\n",
              "      <td>Did your organization adapt to the #blockchain...</td>\n",
              "      <td>8457092</td>\n",
              "      <td>organization adapt blockchain wave sign explor...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3247</th>\n",
              "      <td>1269041313028214786</td>\n",
              "      <td>2020-06-05 23:00:01</td>\n",
              "      <td>The new market landscape has challenged invest...</td>\n",
              "      <td>8457092</td>\n",
              "      <td>new market landscape challenge investment bank...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3248</th>\n",
              "      <td>1269002700064264195</td>\n",
              "      <td>2020-06-05 20:26:35</td>\n",
              "      <td>Earlier this week we expressed our stance agai...</td>\n",
              "      <td>8457092</td>\n",
              "      <td>earlier week express stance racism inequality ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3249</th>\n",
              "      <td>1268996010568749062</td>\n",
              "      <td>2020-06-05 20:00:00</td>\n",
              "      <td>Amidst the #COVID19 pandemic, how can the #ret...</td>\n",
              "      <td>8457092</td>\n",
              "      <td>amidst covid pandemic retailbanking sector mov...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3250 rows × 5 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-52929d80-587c-4629-b046-a5e3a73088aa')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-52929d80-587c-4629-b046-a5e3a73088aa button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-52929d80-587c-4629-b046-a5e3a73088aa');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3. Implementar modelo de tópicos**"
      ],
      "metadata": {
        "id": "o23ktfI1QYf6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_data = [prepare_text_for_lda(i) for i in historical_tweets[\"clean_tweet\"]]\n",
        "\n",
        "dictionary = corpora.Dictionary(text_data)\n",
        "corpus = [dictionary.doc2bow(text) for text in text_data]\n",
        "\n",
        "pickle.dump(corpus, open('corpus.pkl', 'wb'))\n",
        "dictionary.save('dictionary.gensim')\n",
        "\n",
        "\n",
        "NUM_TOPICS = 4\n",
        "ldamodel_1 = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=15)\n",
        "ldamodel_1.save('model6.gensim')\n",
        "topics = ldamodel_1.print_topics(num_words=5)\n",
        "\n",
        "for topic in topics:\n",
        "    print(topic)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWLiCCcbQNWQ",
        "outputId": "892b2112-adfc-4cd6-9695-2664943a52ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0, '0.014*\"organization\" + 0.014*\"learn\" + 0.012*\"deloitte\" + 0.012*\"impact\" + 0.011*\"covid\"')\n",
            "(1, '0.022*\"deloitte\" + 0.009*\"business\" + 0.007*\"leader\" + 0.007*\"challenge\" + 0.007*\"digital\"')\n",
            "(2, '0.031*\"deloitte\" + 0.023*\"global\" + 0.014*\"explore\" + 0.013*\"leader\" + 0.011*\"industry\"')\n",
            "(3, '0.058*\"deloitte\" + 0.031*\"service\" + 0.029*\"leader\" + 0.017*\"gartner\" + 0.011*\"recognize\"')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Visualización**"
      ],
      "metadata": {
        "id": "TP8fIbv5U4gv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyLDAvis"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_MI5t6wVGwi",
        "outputId": "726037a4-0dcd-4ce8-8b97-bdb6f895e338"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyLDAvis\n",
            "  Downloading pyLDAvis-3.3.1.tar.gz (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 4.9 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.8/dist-packages (from pyLDAvis) (3.6.0)\n",
            "Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from pyLDAvis) (1.3.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from pyLDAvis) (1.7.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from pyLDAvis) (1.0.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from pyLDAvis) (2.11.3)\n",
            "Collecting sklearn\n",
            "  Downloading sklearn-0.0.post1.tar.gz (3.6 kB)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.8/dist-packages (from pyLDAvis) (1.21.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from pyLDAvis) (57.4.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from pyLDAvis) (1.2.0)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.8/dist-packages (from pyLDAvis) (2.8.4)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.8/dist-packages (from pyLDAvis) (0.16.0)\n",
            "Collecting funcy\n",
            "  Downloading funcy-1.17-py2.py3-none-any.whl (33 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.2.0->pyLDAvis) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.2.0->pyLDAvis) (2022.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas>=1.2.0->pyLDAvis) (1.15.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.8/dist-packages (from gensim->pyLDAvis) (6.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->pyLDAvis) (2.0.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->pyLDAvis) (3.1.0)\n",
            "Building wheels for collected packages: pyLDAvis, sklearn\n",
            "  Building wheel for pyLDAvis (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyLDAvis: filename=pyLDAvis-3.3.1-py2.py3-none-any.whl size=136898 sha256=202731098cf65314064287c9d7d176b907e21638b5ec1d76677c6c9a78b1ba3d\n",
            "  Stored in directory: /root/.cache/pip/wheels/90/61/ec/9dbe9efc3acf9c4e37ba70fbbcc3f3a0ebd121060aa593181a\n",
            "  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sklearn: filename=sklearn-0.0.post1-py3-none-any.whl size=2344 sha256=754482e150cbbff6e2ed16878067993ae7aa1b2296b929aede78ad89b7eb80d4\n",
            "  Stored in directory: /root/.cache/pip/wheels/14/25/f7/1cc0956978ae479e75140219088deb7a36f60459df242b1a72\n",
            "Successfully built pyLDAvis sklearn\n",
            "Installing collected packages: sklearn, funcy, pyLDAvis\n",
            "Successfully installed funcy-1.17 pyLDAvis-3.3.1 sklearn-0.0.post1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim_models as gensimvis\n",
        "pyLDAvis.enable_notebook()\n",
        "import pickle\n",
        "from gensim import corpora\n",
        "\n",
        "#import pyLDAvis.gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1uYKrPBuU5Wj",
        "outputId": "11c5917d-523c-4bac-ca1d-437f839b62da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/past/types/oldstr.py:5: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working\n",
            "  from collections import Iterable\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dictionary = gensim.corpora.Dictionary.load('dictionary.gensim')\n",
        "corpus = pickle.load(open('corpus.pkl', 'rb'))\n",
        "#ldamodel_1= gensim.models.ldamodel.LdaModel.load('model6.gensim')\n",
        "\n",
        "lda_display_1 = gensimvis.prepare(ldamodel_1, corpus, dictionary, sort_topics=False)\n",
        "pyLDAvis.display(lda_display_1)\n",
        "pyLDAvis.save_html(lda_display_1, 'TopicsUsersHistoricalTweets.html')"
      ],
      "metadata": {
        "id": "k4L7seCqVbeJ",
        "outputId": "10c4025c-211c-480e-f3d5-ef37169c7142",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/smart_open/smart_open_lib.py:496: DeprecationWarning: This function is deprecated.  See https://github.com/RaRe-Technologies/smart_open/blob/develop/MIGRATING_FROM_OLDER_VERSIONS.rst for more information\n",
            "  warnings.warn(message, category=DeprecationWarning)\n"
          ]
        }
      ]
    }
  ]
}